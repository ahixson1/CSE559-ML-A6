# CSE559-ML-A6
Github Repo for ML project

This repository consists of experiments performed for TCR-epitope binding as a part of an academic course project. Most of the code has been borrowed from the Catelmo[1], TCRBert[2], and Bert[3] repositories for training the baseline embedding and the binding affinity prediction models, and experiments to improve the baseline models have been conducted.

[1] Zhang, P., Bang, S., Cai, M., & Lee, H. (2023). Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions. eLife, 12, RP88837. https://doi.org/10.7554/eLife.88837.1
Github: https://github.com/Lee-CBG/catELMo/

[2] Wu, K., Yost, K. E., Daniel, B., Belk, J. A., Xia, Y., Egawa, T., Satpathy, A., Chang, H. Y., & Zou, J. (2021). TCR-BERT: learning the grammar of T-cell receptors for flexible antigen-binding analyses. Preprint. doi:10.1101/2021.11.18.469186. PPR:PPR423052.
Github: https://github.com/wukevin/tcr-bert

[3] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the North American Chapter of the Association for Computational Linguistics.
Github: https://github.com/google-research/bert
